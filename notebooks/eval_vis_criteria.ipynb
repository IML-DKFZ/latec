{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "os.chdir(Path(os.getcwd()).parents[0])\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import sem\n",
    "\n",
    "from src.utils.plot_utils import *\n",
    "\n",
    "\n",
    "def NormalizeData(data, min, max):\n",
    "    return (data - min) / ((max - min) + 0.00000000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Evaluation Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File loading per dataset\n",
    "file_image_inet = \"/image/eval_scores_imagenet.npz\"\n",
    "file_image_oct = \"/image/eval_scores_oct.npz\"\n",
    "file_image_r45 = \"/image/eval_scores_resisc45.npz\"\n",
    "\n",
    "file_volume_adr = \"/volume/eval_scores_adrenalmnist3d.npz\"\n",
    "file_volume_org = \"/volume/eval_scores_organmnist3d.npz\"\n",
    "file_volume_ves = \"/volume/eval_scores_vesselmnist3d.npz\"\n",
    "\n",
    "file_pc_coma = \"/point_cloud/eval_scores_coma.npz\"\n",
    "file_pc_m40 = \"/point_cloud/eval_scores_modelnet40.npz\"\n",
    "file_pc_shpn = \"/point_cloud/eval_scores_shapenet.npz\"\n",
    "\n",
    "file_loc = os.getcwd() + \"/data/evaluation_scores\"\n",
    "\n",
    "file = np.load(file_loc + file_image_inet, allow_pickle=True)\n",
    "arr_image_inet = [file[\"arr_0\"], file[\"arr_1\"], file[\"arr_2\"]]\n",
    "file = np.load(file_loc + file_image_oct, allow_pickle=True)\n",
    "arr_image_oct = [file[\"arr_0\"], file[\"arr_1\"], file[\"arr_2\"]]\n",
    "file = np.load(file_loc + file_image_r45, allow_pickle=True)\n",
    "arr_image_r45 = [file[\"arr_0\"], file[\"arr_1\"], file[\"arr_2\"]]\n",
    "\n",
    "file = np.load(file_loc + file_volume_adr, allow_pickle=True)\n",
    "arr_volume_adr = [file[\"arr_0\"], file[\"arr_1\"], file[\"arr_2\"]]\n",
    "file = np.load(file_loc + file_volume_org, allow_pickle=True)\n",
    "arr_volume_org = [file[\"arr_0\"], file[\"arr_1\"], file[\"arr_2\"]]\n",
    "file = np.load(file_loc + file_volume_ves, allow_pickle=True)\n",
    "arr_volume_ves = [file[\"arr_0\"], file[\"arr_1\"], file[\"arr_2\"]]\n",
    "\n",
    "file = np.load(file_loc + file_pc_coma, allow_pickle=True)\n",
    "arr_pc_coma = [file[\"arr_0\"], file[\"arr_1\"], file[\"arr_2\"]]\n",
    "file = np.load(file_loc + file_pc_m40, allow_pickle=True)\n",
    "arr_pc_m40 = [file[\"arr_0\"], file[\"arr_1\"], file[\"arr_2\"]]\n",
    "file = np.load(file_loc + file_pc_shpn, allow_pickle=True)\n",
    "arr_pc_shpn = [file[\"arr_0\"], file[\"arr_1\"], file[\"arr_2\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Metric Variance against Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint, levene\n",
    "import scipy\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "arr_sign_test = np.empty(\n",
    "    [3, 3, 14, 3], dtype=int  # dataset, model, xai methods, eval_criteria\n",
    ")\n",
    "random_sample = randint.rvs(0, 15, size=999999)\n",
    "\n",
    "\n",
    "for crit in range(3):\n",
    "    for modality in range(2):\n",
    "        for method in range(14):\n",
    "            for model in range(3):\n",
    "                for dataset in range(3):\n",
    "                    metric_sample = [\n",
    "                        arr_ranking[dataset, model, method, :10],\n",
    "                        arr_ranking[dataset, model, method, 10:17],\n",
    "                        arr_ranking[dataset, model, method, 17:20],\n",
    "                    ][crit]\n",
    "\n",
    "                    if metric_sample.var() > 16.25:\n",
    "                        arr_sign_test[dataset, model, method, crit] = 0\n",
    "                    else:\n",
    "                        test_pvalue = levene(\n",
    "                            random_sample, metric_sample, center=\"median\"\n",
    "                        ).pvalue\n",
    "                        arr_sign_test[dataset, model, method, crit] = (\n",
    "                            1 if test_pvalue < alpha else 0\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_sign_test = np.empty([14, 3], dtype=float)  # xai methods, eval_criteria\n",
    "\n",
    "for method in range(14):\n",
    "    for crit in range(3):\n",
    "        table_sign_test[method, crit] = np.round(\n",
    "            arr_sign_test[:, :, method, crit].mean(), 2\n",
    "        )\n",
    "\n",
    "table_sign_test = pd.DataFrame(table_sign_test).transpose()\n",
    "\n",
    "table_sign_test = table_sign_test.append(\n",
    "    pd.DataFrame(\n",
    "        np.round(\n",
    "            np.average(table_sign_test, axis=0, weights=[0.5, 0.35, 0.15]), 2\n",
    "        ).reshape(1, -1)\n",
    "    ),\n",
    "    ignore_index=True,\n",
    ")\n",
    "table_sign_test[\"mean\"] = np.round(table_sign_test.mean(axis=1), 2)\n",
    "\n",
    "table_sign_test.columns = [\n",
    "    \"OC\",\n",
    "    \"LI\",\n",
    "    \"KS\",\n",
    "    \"VG\",\n",
    "    \"IxG\",\n",
    "    \"GB\",\n",
    "    \"GC\",\n",
    "    \"SC\",\n",
    "    \"C+\",\n",
    "    \"IG\",\n",
    "    \"EG\",\n",
    "    \"DL\",\n",
    "    \"DLS\",\n",
    "    \"LRP\",\n",
    "    \"Average\",\n",
    "]\n",
    "\n",
    "table_sign_test.index = [\"Faithfulness\", \"Robustness\", \"Complexity\", \"Weighted Average\"]\n",
    "table_sign_test.to_csv(\n",
    "    os.getcwd().split(\"src\")[0] + \"data/figures/variance_sign_test.csv\"\n",
    ")\n",
    "table_sign_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Disagreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Ranking\n",
    "arr_image = [arr_image_inet, arr_image_oct, arr_image_r45]\n",
    "arr_volume = [arr_volume_adr, arr_volume_org, arr_volume_ves]\n",
    "arr_pc = [arr_pc_coma, arr_pc_m40, arr_pc_shpn]\n",
    "arr_modalities = [arr_image, arr_volume, arr_pc]\n",
    "\n",
    "arr_ranking = np.empty([3, 3, 14, 20], dtype=float)  # , dataset, model, xai, eval\n",
    "arr_ranking[:] = np.nan\n",
    "\n",
    "bup_order = [0, 1, 2, 4, 5, 7, 9, 12, 17]\n",
    "\n",
    "\n",
    "for dataset in range(3):\n",
    "    for model in range(3):\n",
    "        for xai in range(14):\n",
    "            for eval in range(20):\n",
    "                ranking = np.median(\n",
    "                    arr_modalities[0][dataset][model][:14, eval, :], -1\n",
    "                ).argsort()  # compute ranking based on median obs score\n",
    "                if eval in bup_order:\n",
    "                    ranking = ranking[\n",
    "                        ::-1\n",
    "                    ]  # reverse ranking to bottom up if larger is better\n",
    "\n",
    "                pos = (\n",
    "                    ranking.argsort()[xai] + 1\n",
    "                )  # get rankin position of xai method (+1 so ranking starts at 1 and not 0)\n",
    "                arr_ranking[dataset, model, xai, eval] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "colors = list(map(px.colors.qualitative.G10.__getitem__, [0, 2, 3, 4]))\n",
    "metrics_faith = [\n",
    "    \"FC\",\n",
    "    \"FE\",\n",
    "    \"MC\",\n",
    "    \"PF\",\n",
    "    \"RP\",\n",
    "    \"INS\",\n",
    "    \"DEL\",\n",
    "    \"IROF\",\n",
    "    \"ROAD\",\n",
    "    \"SUF\",\n",
    "    \"INF\",\n",
    "]\n",
    "metrics_robust = [\n",
    "    \"LLE\",\n",
    "    \"MS\",\n",
    "    \"CON\",\n",
    "    \"RIS\",\n",
    "    \"ROS\",\n",
    "    \"RRS\",\n",
    "]\n",
    "metrics_complex = [\"SP\", \"CP\", \"ECP\"]\n",
    "\n",
    "methods = [1, 6, 9, 12]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=3,\n",
    "    subplot_titles=[\"Faithfulness Metrics\", \"Robustness Metrics\", \"Complexity Metrics\"],\n",
    "    column_widths=[0.5, 0.35, 0.15],\n",
    "    shared_yaxes=True,\n",
    ")\n",
    "\n",
    "df_table = pd.DataFrame(arr_ranking[0, 0, :, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 16]].T)\n",
    "df_table.index = [\n",
    "    \"OC\",\n",
    "    \"LIME\",\n",
    "    \"KS\",\n",
    "    \"VG\",\n",
    "    \"IxG\",\n",
    "    \"GB\",\n",
    "    \"GC\",\n",
    "    \"SC\",\n",
    "    \"C+\",\n",
    "    \"IG\",\n",
    "    \"EG\",\n",
    "    \"DL\",\n",
    "    \"DLS\",\n",
    "    \"LRP\",\n",
    "]\n",
    "\n",
    "faith = df_table.iloc[methods, :].transpose()\n",
    "faith.iloc[10, 0] = 12\n",
    "faith.iloc[10, 3] = 4\n",
    "\n",
    "df_table = pd.DataFrame(arr_ranking[0, 0, :, 10:16])\n",
    "robust = df_table.iloc[methods, :].transpose()\n",
    "\n",
    "df_table = pd.DataFrame(arr_ranking[0, 0, :, 17:20])\n",
    "complex = df_table.iloc[methods, :].transpose()\n",
    "\n",
    "\n",
    "for i in range(len(faith.columns)):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=metrics_faith,\n",
    "            y=faith.iloc[:, i],\n",
    "            mode=\"lines+markers\",\n",
    "            name=faith.columns[i],\n",
    "            marker=dict(color=colors[i], size=8),\n",
    "        ),\n",
    "        col=1,\n",
    "        row=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[\"Average\"],\n",
    "            y=[np.mean(faith.iloc[:, i])],\n",
    "            mode=\"markers+text\",\n",
    "            text=[str(np.mean(faith.iloc[:, i]).round(2))],\n",
    "            textposition=\"middle left\",\n",
    "            showlegend=False,\n",
    "            marker=dict(color=colors[i], size=8, symbol=\"square\"),\n",
    "        ),\n",
    "        col=1,\n",
    "        row=1,\n",
    "    )\n",
    "\n",
    "for i in range(len(robust.columns)):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=metrics_robust,\n",
    "            y=robust.iloc[:, i],\n",
    "            mode=\"lines+markers\",\n",
    "            showlegend=False,\n",
    "            marker=dict(color=colors[i], size=8),\n",
    "        ),\n",
    "        col=2,\n",
    "        row=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[\"Average\"],\n",
    "            y=[np.mean(robust.iloc[:, i]), 1],\n",
    "            mode=\"markers+text\",\n",
    "            text=[str(np.round(np.mean(robust.iloc[:, i]), 1))],\n",
    "            textposition=\"middle left\",\n",
    "            showlegend=False,\n",
    "            marker=dict(color=colors[i], size=8, symbol=\"square\"),\n",
    "        ),\n",
    "        col=2,\n",
    "        row=1,\n",
    "    )\n",
    "\n",
    "for i in range(len(complex.columns)):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=metrics_complex,\n",
    "            y=complex.iloc[:, i],\n",
    "            mode=\"lines+markers\",\n",
    "            showlegend=False,\n",
    "            marker=dict(color=colors[i], size=8),\n",
    "        ),\n",
    "        col=3,\n",
    "        row=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[\"Average\"],\n",
    "            y=[np.mean(complex.iloc[:, i])],\n",
    "            mode=\"markers+text\",\n",
    "            text=[str(np.round(np.mean(complex.iloc[:, i])))],\n",
    "            textposition=\"middle left\",\n",
    "            showlegend=False,\n",
    "            marker=dict(color=colors[i], size=8, symbol=\"square\"),\n",
    "        ),\n",
    "        col=3,\n",
    "        row=1,\n",
    "    )\n",
    "\n",
    "fig.update_yaxes(\n",
    "    autorange=\"reversed\",\n",
    "    range=[1, 14],\n",
    "    tickvals=[1, 5, 10, 14],\n",
    "    zeroline=False,\n",
    "    showticklabels=True,\n",
    ")\n",
    "\n",
    "fig.update_yaxes(title=\"Rank\", col=1, row=1)\n",
    "\n",
    "fig.update_xaxes(tickangle=35)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=400,\n",
    "    width=1500,\n",
    "    legend_title_text=\"XAI Method\",\n",
    "    template=\"plotly_white\",\n",
    "    font=dict(\n",
    "        family=\"Helvetica\",\n",
    "        color=\"#000000\",\n",
    "        size=12,\n",
    "    ),\n",
    "    title_font=dict(family=\"Helvetica\", color=\"#000000\", size=12),\n",
    ")\n",
    "\n",
    "fig = left_align_facet_plot_titles(fig)\n",
    "fig.write_image(\n",
    "    os.getcwd().split(\"src\")[0] + \"data/figures/meta_eval_example.png\", scale=2\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Ranking Distance Matricies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "list = []\n",
    "\n",
    "for model in range(3):\n",
    "    for dataset in range(3):\n",
    "        for method in range(14):\n",
    "            list.append(\n",
    "                # np.corrcoef(arr_ranking[dataset, model, :, :11].T)\n",
    "                distance_matrix(\n",
    "                    np.expand_dims(arr_ranking[dataset, model, method, :11], 0).T,\n",
    "                    np.expand_dims(arr_ranking[dataset, model, method, :11], 0).T,\n",
    "                )\n",
    "            )\n",
    "\n",
    "dist_matrix = np.round(np.mean(np.array(list[:42]), 0), 2)\n",
    "dist_matrix = pd.DataFrame(dist_matrix)\n",
    "dist_faith_resnet50 = dist_matrix.where(\n",
    "    np.tril(np.ones(dist_matrix.shape)).astype(bool)\n",
    ")\n",
    "\n",
    "dist_matrix = np.round(np.mean(np.array(list[42:84]), 0), 2)\n",
    "dist_matrix = pd.DataFrame(dist_matrix)\n",
    "dist_faith_effnetb0 = dist_matrix.where(\n",
    "    np.tril(np.ones(dist_matrix.shape)).astype(bool)\n",
    ")\n",
    "\n",
    "dist_matrix = np.round(np.mean(np.array(list[84:]), 0), 2)\n",
    "dist_matrix = pd.DataFrame(dist_matrix)\n",
    "dist_faith_vit = dist_matrix.where(np.tril(np.ones(dist_matrix.shape)).astype(bool))\n",
    "\n",
    "list = []\n",
    "\n",
    "for model in range(3):\n",
    "    for dataset in range(3):\n",
    "        for method in range(14):\n",
    "            list.append(\n",
    "                # np.corrcoef(arr_ranking[dataset, model, :, 11:17].T)\n",
    "                distance_matrix(\n",
    "                    np.expand_dims(arr_ranking[dataset, model, method, 11:17], 0).T,\n",
    "                    np.expand_dims(arr_ranking[dataset, model, method, 11:17], 0).T,\n",
    "                )\n",
    "            )\n",
    "\n",
    "dist_matrix = np.round(np.mean(np.array(list[:42]), 0), 2)\n",
    "dist_matrix = pd.DataFrame(dist_matrix)\n",
    "dist_robust_resnet50 = dist_matrix.where(\n",
    "    np.tril(np.ones(dist_matrix.shape)).astype(bool)\n",
    ")\n",
    "\n",
    "dist_matrix = np.round(np.mean(np.array(list[42:84]), 0), 2)\n",
    "dist_matrix = pd.DataFrame(dist_matrix)\n",
    "dist_robust_effnetb0 = dist_matrix.where(\n",
    "    np.tril(np.ones(dist_matrix.shape)).astype(bool)\n",
    ")\n",
    "\n",
    "dist_matrix = np.round(np.mean(np.array(list[84:]), 0), 2)\n",
    "dist_matrix = pd.DataFrame(dist_matrix)\n",
    "dist_robust_vit = dist_matrix.where(np.tril(np.ones(dist_matrix.shape)).astype(bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "metrics_faith = [\n",
    "    \"FC\",\n",
    "    \"FE\",\n",
    "    \"MC\",\n",
    "    \"PF\",\n",
    "    \"RP\",\n",
    "    \"INS\",\n",
    "    \"DEL\",\n",
    "    \"IROF\",\n",
    "    \"ROAD\",\n",
    "    \"SUF\",\n",
    "    \"INF\",\n",
    "]\n",
    "\n",
    "metrics_robust = [\n",
    "    \"LLE\",\n",
    "    \"MS\",\n",
    "    \"CON\",\n",
    "    \"RIS\",\n",
    "    \"ROS\",\n",
    "    \"RRS\",\n",
    "]\n",
    "metrics_complex = [\"SP\", \"CP\", \"ECP\"]\n",
    "\n",
    "colors = px.colors.qualitative.G10\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=3,\n",
    "    subplot_titles=[\n",
    "        \"ResNet50\",\n",
    "        \"EfficientNetb0\",\n",
    "        \"DeiT ViT\",\n",
    "        \"ResNet50\",\n",
    "        \"EfficientNetb0\",\n",
    "        \"DeiT ViT\",\n",
    "    ],\n",
    "    # column_widths=[0.7, 0.3],\n",
    ")\n",
    "\n",
    "for i in range(3):\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=[dist_faith_resnet50, dist_faith_effnetb0, dist_faith_vit][i],\n",
    "            x=metrics_faith,\n",
    "            y=metrics_faith,\n",
    "            texttemplate=\"%{z}\",\n",
    "            colorscale=\"RdYlGn_r\",\n",
    "            reversescale=True,\n",
    "            zmin=-1,\n",
    "            zmax=1,\n",
    "            colorbar=dict(ticks=\"outside\", thickness=10),\n",
    "        ),\n",
    "        col=i + 1,\n",
    "        row=1,\n",
    "    )\n",
    "\n",
    "for i in range(3):\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=[dist_robust_resnet50, dist_robust_effnetb0, dist_robust_vit][i],\n",
    "            x=metrics_robust,\n",
    "            y=metrics_robust,\n",
    "            texttemplate=\"%{z}\",\n",
    "            colorscale=\"RdYlGn_r\",\n",
    "            reversescale=True,\n",
    "            zmin=-1,\n",
    "            zmax=1,\n",
    "            colorbar=dict(ticks=\"outside\", thickness=10),\n",
    "        ),\n",
    "        col=i + 1,\n",
    "        row=2,\n",
    "    )\n",
    "\n",
    "fig.update_yaxes(showgrid=False)\n",
    "fig.update_yaxes(title=\"Faithfulness Metrics\", row=1, col=1)\n",
    "fig.update_yaxes(title=\"Robustness Metrics\", row=2, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    font=dict(family=\"Helvetica\", color=\"#000000\", size=13),\n",
    "    template=\"plotly_white\",\n",
    "    height=900,\n",
    "    width=1600,\n",
    "    title_font=dict(family=\"Helvetica\", color=\"#000000\", size=14),\n",
    ")\n",
    "\n",
    "fig = left_align_facet_plot_titles(fig)\n",
    "\n",
    "fig.write_image(\n",
    "    os.getcwd().split(\"src\")[0] + \"/data/figures/meta_eval_correlation.png\", scale=3\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
